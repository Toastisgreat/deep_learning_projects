{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 18:55:27.558451: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-03 18:55:27.576832: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-03 18:55:27.688005: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-03 18:55:27.689345: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 18:55:28.549311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  []\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from random import choice\n",
    "import gym\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym import spaces\n",
    "from copy import deepcopy\n",
    "import pygame.font\n",
    "import os\n",
    "pygame.font.init()\n",
    "pygame.init()\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2315\n"
     ]
    }
   ],
   "source": [
    "validwords = []\n",
    "with open('resources/wordlist.txt') as wordlist:\n",
    "    for line in wordlist:\n",
    "        # clean the line\n",
    "        text = line.replace('\\n', '')\n",
    "        validwords.append(text.lower().strip())\n",
    "print(len(validwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wordle:\n",
    "    def __init__(self, word, rows=6, letters=5):\n",
    "        self.g_count = 0\n",
    "        self.word = word\n",
    "        self.w_hash_table = {}\n",
    "        if word is not None:\n",
    "            for x, l in enumerate(word):\n",
    "                if l in self.w_hash_table:\n",
    "                    self.w_hash_table[l]['count'] += 1\n",
    "                    self.w_hash_table[l]['pos'].append(x)\n",
    "                else:\n",
    "                    self.w_hash_table[l] = {'count':1, 'pos':[x]}\n",
    "        self.rows = rows\n",
    "        self.letters = letters\n",
    "        self.board = [['' for _ in range(letters)] for _ in range(rows)]\n",
    "        self.colours = [['B' for _ in range(letters)] for _ in range(rows)]\n",
    "        self.alph = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "\n",
    "    def is_end(self):\n",
    "        if self.board[-1] != ['' for _ in range(self.letters)]:\n",
    "            return True\n",
    "        else:\n",
    "            r = self.game_result()\n",
    "            if r[0] == True:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def game_result(self):\n",
    "        win = (False, 100)\n",
    "        for i, r in enumerate(self.board):\n",
    "            if self.word == ''.join(r):\n",
    "                win = (True, i)\n",
    "                break\n",
    "        return win\n",
    "\n",
    "    def update_board(self, u_inp):\n",
    "        w_hash_table = deepcopy(self.w_hash_table)\n",
    "        i_hash_table = {}\n",
    "        for x, l in enumerate(str(u_inp).upper()):\n",
    "            self.board[self.g_count][x] = l\n",
    "            if l in i_hash_table:\n",
    "                i_hash_table[l].append(x)\n",
    "            else:\n",
    "                i_hash_table[l] = [x]\n",
    "        colours = {'G':[],'B':[],'Y':[]}\n",
    "        for l in i_hash_table:\n",
    "            if l in w_hash_table:\n",
    "                g_hold = []\n",
    "                for p in i_hash_table[l]:\n",
    "                    if p in w_hash_table[l]['pos']:\n",
    "                        g_hold.append(p)\n",
    "                for p in g_hold:\n",
    "                    i_hash_table[l].remove(p)\n",
    "                colours['G'] += g_hold\n",
    "                if len(g_hold) < w_hash_table[l]['count']:\n",
    "                    y_hold = []\n",
    "                    for p in i_hash_table[l]:\n",
    "                        y_hold.append(p)\n",
    "                        if len(y_hold) == w_hash_table[l]['count']:\n",
    "                            break\n",
    "                    for p in y_hold:\n",
    "                        i_hash_table[l].remove(p)\n",
    "                    colours['Y'] += y_hold\n",
    "                for p in i_hash_table[l]:\n",
    "                    colours['B'].append(p)\n",
    "            else:\n",
    "                colours['B'] += i_hash_table[l]\n",
    "                i_hash_table[l] = []\n",
    "        for c in colours:\n",
    "            for p in colours[c]:\n",
    "                self.colours[self.g_count][p] = c\n",
    "        self.g_count += 1\n",
    "\n",
    "    def valid_guess(self, u_inp):\n",
    "        if len(u_inp) == 5 and False not in [False for s in str(u_inp).upper() if s not in self.alph]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "class WordleEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SCREEN_DIM = 500\n",
    "    GREEN = \"#6aaa64\"\n",
    "    YELLOW = \"#c9b458\"\n",
    "    GREY = \"#787c7e\"\n",
    "    OUTLINE = \"#d3d6da\"\n",
    "    FILLED_OUTLINE = \"#878a8c\"\n",
    "\n",
    "    def __init__(self, answers, logging=False):\n",
    "        self.logging = logging\n",
    "        self.answers = pd.DataFrame(answers)\n",
    "        self.answers.columns = ['words']\n",
    "        self.screen = None\n",
    "        self.isopen = False\n",
    "        self.GUESSES = 6\n",
    "        self.LETTERS = 5\n",
    "        self.WORD = self.answers['words'].sample(n=1).tolist()[0].upper()\n",
    "        self.WORDLE = Wordle(self.WORD, self.GUESSES, self.LETTERS)\n",
    "        self.alpha = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "        self.colors = ['B', 'Y', 'G']\n",
    "        self.is_game_over = False\n",
    "        self.guessed_words = []\n",
    "        self.blank_letters = []\n",
    "\n",
    "        # our action space is the total amount of possible words to guess\n",
    "        self.action_space = spaces.Discrete(len(answers))\n",
    "        #our observation space is the current wordle board in form of (letter, color) with 5x6 (5 letters, 6 guesses)\n",
    "        #modified to work with gym/baselines\n",
    "        #same thing basically, only 0-26 is '' to z and 27-29 is B, Y, G\n",
    "        # first 6 rows are guesses and last 6 rows are colors\n",
    "        # changed shape to be 3 dimensions so that we can apply conv2d layers to it\n",
    "        # at some point we should try to normalize the obs space\n",
    "        # since right now its on a 0-29 scale instead of a 0-1.\n",
    "        self.observation_space = spaces.Box(low=0, high=29, shape=(1,12,5), dtype='int32')\n",
    "        self.current_episode = -1\n",
    "        self.episode_memory: list[any] = []\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.is_game_over:\n",
    "            return RuntimeError('Episode is already done')\n",
    "        guess = self._take_action(action)\n",
    "        reward = self._get_reward(guess)\n",
    "        self.guessed_words.append(guess.upper())\n",
    "        observation = self._get_observation()\n",
    "        res = self.WORDLE.colours[self.WORDLE.g_count-1]\n",
    "        self.blank_letters.extend([ l for i,l in enumerate(guess) if res[i] == 'B' and l not in self.blank_letters])\n",
    "        return observation, reward, self.is_game_over, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_episode = -1\n",
    "        self.episode_memory.append([])\n",
    "        self.is_game_over = False\n",
    "        self.WORD = self.answers['words'].sample(n=1).tolist()[0].upper()\n",
    "        self.WORDLE = Wordle(self.WORD, self.GUESSES, self.LETTERS)\n",
    "        self.guessed_words = []\n",
    "        self.blank_letters = []\n",
    "        if self.logging:\n",
    "            #print(self.WORDLE.word)\n",
    "            pass\n",
    "        self.close()\n",
    "        return self._get_observation()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pygame.init()\n",
    "        pygame.display.init()\n",
    "        self.screen = pygame.display.set_mode((self.SCREEN_DIM, self.SCREEN_DIM))\n",
    "        font = pygame.font.Font('freesansbold.ttf', 30)\n",
    "        for col in range(0, 5):\n",
    "            for row in range(0, 6):\n",
    "                pygame.draw.rect(self.screen, self.OUTLINE, [col * 100 + 12, row * 100 + 12, 75, 75], 3, 5)\n",
    "                color = self.GREEN if self.WORDLE.colours[row][col] == 'G' else self.YELLOW if self.WORDLE.colours[row][col] == 'Y' else self.GREY\n",
    "                piece_text = font.render(self.WORDLE.board[row][col], True, color)\n",
    "                self.screen.blit(piece_text, (col * 100 + 30, row * 100 + 25))\n",
    "        #pygame.draw.rect(screen, self.GREEN, [5, turn * 100 + 5, WIDTH - 10, 90], 3, 5)\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.flip()             \n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # turn action into guess\n",
    "        guess = self.answers['words'][action].upper()\n",
    "        self.episode_memory[self.current_episode].append(guess)\n",
    "        if self.logging:\n",
    "            print(guess)\n",
    "            pass\n",
    "        self.WORDLE.update_board(guess)\n",
    "        res = self.WORDLE.colours[self.WORDLE.g_count-1]\n",
    "        self.is_game_over = self.WORDLE.word == guess.upper() or self.WORDLE.g_count == self.GUESSES\n",
    "        \n",
    "        if self.is_game_over and self.logging:\n",
    "            print(f'Guessed in : {len(self.guessed_words)} \\nWords: ', end='')\n",
    "            print(*self.guessed_words, sep=\",\")\n",
    "            print(f'Answer: {self.WORD}')\n",
    "        if self.WORDLE.word == guess:\n",
    "            print(f'Guessed {self.WORDLE.word} in {self.WORDLE.g_count} guesses!')\n",
    "        return guess\n",
    "    def _get_reward(self, guess):\n",
    "        result, tries = self.WORDLE.game_result()\n",
    "        rewards = np.zeros(5)\n",
    "        #heavily penealize guessing the same word multiple times\n",
    "        #If a word isn't the right guess, we shouldn't guess it again\n",
    "        #could do the same thing for letters, as if a letter is blank(grey)\n",
    "        # then the only reason to use a word with a letter in it\n",
    "        # is to check other letter posistions\n",
    "        #so it shouldn't be a heavy penalty but it should be a penalty\n",
    "        for i,c in enumerate(self.WORDLE.colours[self.WORDLE.g_count-1]):\n",
    "            if c == self.colors[2]:\n",
    "                rewards[i] = 3\n",
    "            elif c == self.colors[1]:\n",
    "                rewards[i] = 2\n",
    "            elif c == self.colors[0]:\n",
    "                rewards[i] = 1\n",
    "        #check guesses up to and including our current guess\n",
    "        if self.logging:\n",
    "            print(self.WORD)\n",
    "            print(rewards)\n",
    "        reward = np.mean(rewards)\n",
    "        '''\n",
    "        for g in range(self.WORDLE.g_count):\n",
    "            word = self.WORDLE.board[g]\n",
    "            current = ''.join(word)\n",
    "            if guess in self.guessed_words:\n",
    "                print('did this')\n",
    "                return 0\n",
    "                print('0')\n",
    "            for l in word: \n",
    "                if l in self.blank_letters:\n",
    "                    reward -= 0.3'''\n",
    "        if guess in self.guessed_words:\n",
    "            return 0\n",
    "        for l in guess:\n",
    "            if l in self.blank_letters:\n",
    "                reward -= 1\n",
    "    \n",
    "        return reward\n",
    "\n",
    "    def _get_observation(self):\n",
    "        board = np.array(self.WORDLE.board) #2d array of 5x6\n",
    "        colors = np.array(self.WORDLE.colours) #2d array of 5x6\n",
    "        results = np.vstack((board, colors)) #stacks boards and colors by rows resulting in a 2d array of 5x12\n",
    "        convertletterstonum = lambda letter: [self.alpha.index(l) + 1 if l in self.alpha else 0 for l in letter]\n",
    "        convertcolortonum = lambda color: [self.colors.index(c)+27 for c in color]\n",
    "        guesses = np.array([convertletterstonum(l) if i <=5 else convertcolortonum(l) for i, l in enumerate(results)])\n",
    "        guesses3d = np.expand_dims(guesses, axis=0)\n",
    "        if self.logging:\n",
    "            pass\n",
    "            #print(np.shape(guesses))\n",
    "            #print(np.shape(guesses3d))\n",
    "        return guesses3d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 5)\n",
      "2315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = WordleEnv(validwords)\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(states)\n",
    "print(actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordleNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "        bias_layer = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=(12,5), stride=2, padding=(7,7))\n",
    "        with torch.no_grad():\n",
    "            bias_layer.bias.fill_(1.)\n",
    "        self.online = nn.Sequential(\n",
    "            bias_layer,\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = deepcopy(self.online)\n",
    "        \n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordleAgent:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(10000000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.net = WordleNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 50\n",
    "        self.exploration_rate_decay = 1-1e-20\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Wordle Net\n",
    "\n",
    "\n",
    "        #Learning\n",
    "        self.gamma = 0.9\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        #Completion\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0).float()\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        #decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        #Stores the environment to self.memory\n",
    "\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "    def recall(self):\n",
    "        #Retrieve a batch of experiences from self.memory\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        state = state.float()\n",
    "        next_state = next_state.float()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir, silent=False):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "        # Make output\n",
    "        self.silent = silent\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 5)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        if not self.silent:\n",
    "            print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "            )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Episode 0 - Step 6 - Epsilon 50.0 - Mean Reward -6.0 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.006 - Time 2023-07-03T18:57:10\n",
      "Episode 20 - Step 126 - Epsilon 50.0 - Mean Reward -2.152 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.367 - Time 2023-07-03T18:57:11\n",
      "Guessed ASKEW in 1 guesses!\n",
      "Episode 40 - Step 241 - Epsilon 50.0 - Mean Reward -1.824 - Mean Length 5.87805 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.404 - Time 2023-07-03T18:57:11\n",
      "Episode 60 - Step 361 - Epsilon 50.0 - Mean Reward -1.987 - Mean Length 5.91803 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.425 - Time 2023-07-03T18:57:11\n",
      "Episode 80 - Step 481 - Epsilon 50.0 - Mean Reward -1.822 - Mean Length 5.93827 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.4 - Time 2023-07-03T18:57:12\n",
      "Episode 100 - Step 601 - Epsilon 50.0 - Mean Reward -1.63 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.418 - Time 2023-07-03T18:57:12\n",
      "Episode 120 - Step 721 - Epsilon 50.0 - Mean Reward -1.806 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.404 - Time 2023-07-03T18:57:13\n",
      "Episode 140 - Step 841 - Epsilon 50.0 - Mean Reward -1.612 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.431 - Time 2023-07-03T18:57:13\n",
      "Guessed APNEA in 3 guesses!\n",
      "Episode 160 - Step 958 - Epsilon 50.0 - Mean Reward -1.404 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.494 - Time 2023-07-03T18:57:13\n",
      "Episode 180 - Step 1078 - Epsilon 50.0 - Mean Reward -1.602 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.779 - Time 2023-07-03T18:57:14\n",
      "Episode 200 - Step 1198 - Epsilon 50.0 - Mean Reward -1.61 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.403 - Time 2023-07-03T18:57:15\n",
      "Episode 220 - Step 1318 - Epsilon 50.0 - Mean Reward -1.426 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.394 - Time 2023-07-03T18:57:15\n",
      "Episode 240 - Step 1438 - Epsilon 50.0 - Mean Reward -1.618 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.4 - Time 2023-07-03T18:57:15\n",
      "Episode 260 - Step 1558 - Epsilon 50.0 - Mean Reward -1.458 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.407 - Time 2023-07-03T18:57:16\n",
      "Episode 280 - Step 1678 - Epsilon 50.0 - Mean Reward -1.364 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.399 - Time 2023-07-03T18:57:16\n",
      "Guessed TORCH in 5 guesses!\n",
      "Episode 300 - Step 1797 - Epsilon 50.0 - Mean Reward -1.668 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.461 - Time 2023-07-03T18:57:17\n",
      "Episode 320 - Step 1917 - Epsilon 50.0 - Mean Reward -1.704 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.452 - Time 2023-07-03T18:57:17\n",
      "Episode 340 - Step 2037 - Epsilon 50.0 - Mean Reward -1.832 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.435 - Time 2023-07-03T18:57:18\n",
      "Episode 360 - Step 2157 - Epsilon 50.0 - Mean Reward -2.104 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.416 - Time 2023-07-03T18:57:18\n",
      "Episode 380 - Step 2277 - Epsilon 50.0 - Mean Reward -2.238 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.431 - Time 2023-07-03T18:57:18\n",
      "Episode 400 - Step 2397 - Epsilon 50.0 - Mean Reward -2.176 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.461 - Time 2023-07-03T18:57:19\n",
      "Episode 420 - Step 2517 - Epsilon 50.0 - Mean Reward -2.192 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.482 - Time 2023-07-03T18:57:19\n",
      "Episode 440 - Step 2637 - Epsilon 50.0 - Mean Reward -2.32 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.426 - Time 2023-07-03T18:57:20\n",
      "Episode 460 - Step 2757 - Epsilon 50.0 - Mean Reward -2.344 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.421 - Time 2023-07-03T18:57:20\n",
      "Episode 480 - Step 2877 - Epsilon 50.0 - Mean Reward -2.148 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.392 - Time 2023-07-03T18:57:21\n",
      "Episode 500 - Step 2997 - Epsilon 50.0 - Mean Reward -1.904 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.393 - Time 2023-07-03T18:57:21\n",
      "Episode 520 - Step 3117 - Epsilon 50.0 - Mean Reward -1.686 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.593 - Time 2023-07-03T18:57:22\n",
      "Episode 540 - Step 3237 - Epsilon 50.0 - Mean Reward -1.564 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.397 - Time 2023-07-03T18:57:22\n",
      "Episode 560 - Step 3357 - Epsilon 50.0 - Mean Reward -1.58 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.42 - Time 2023-07-03T18:57:22\n",
      "Episode 580 - Step 3477 - Epsilon 50.0 - Mean Reward -1.696 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.488 - Time 2023-07-03T18:57:23\n",
      "Episode 600 - Step 3597 - Epsilon 50.0 - Mean Reward -1.946 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.506 - Time 2023-07-03T18:57:23\n",
      "Episode 620 - Step 3717 - Epsilon 50.0 - Mean Reward -2.2 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.491 - Time 2023-07-03T18:57:24\n",
      "Episode 640 - Step 3837 - Epsilon 50.0 - Mean Reward -2.15 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.444 - Time 2023-07-03T18:57:24\n",
      "Episode 660 - Step 3957 - Epsilon 50.0 - Mean Reward -2.114 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.454 - Time 2023-07-03T18:57:25\n",
      "Episode 680 - Step 4077 - Epsilon 50.0 - Mean Reward -1.96 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.423 - Time 2023-07-03T18:57:25\n",
      "Episode 700 - Step 4197 - Epsilon 50.0 - Mean Reward -1.928 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.413 - Time 2023-07-03T18:57:26\n",
      "Episode 720 - Step 4317 - Epsilon 50.0 - Mean Reward -1.84 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.412 - Time 2023-07-03T18:57:26\n",
      "Episode 740 - Step 4437 - Epsilon 50.0 - Mean Reward -1.91 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.402 - Time 2023-07-03T18:57:26\n",
      "Episode 760 - Step 4557 - Epsilon 50.0 - Mean Reward -1.922 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.403 - Time 2023-07-03T18:57:27\n",
      "Episode 780 - Step 4677 - Epsilon 50.0 - Mean Reward -1.874 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.407 - Time 2023-07-03T18:57:27\n",
      "Episode 800 - Step 4797 - Epsilon 50.0 - Mean Reward -1.96 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.415 - Time 2023-07-03T18:57:28\n",
      "Episode 820 - Step 4917 - Epsilon 50.0 - Mean Reward -2.044 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.411 - Time 2023-07-03T18:57:28\n",
      "Episode 840 - Step 5037 - Epsilon 50.0 - Mean Reward -2.214 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.379 - Time 2023-07-03T18:57:29\n",
      "Guessed PARER in 3 guesses!\n",
      "Episode 860 - Step 5154 - Epsilon 50.0 - Mean Reward -2.306 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.583 - Time 2023-07-03T18:57:29\n",
      "Episode 880 - Step 5274 - Epsilon 50.0 - Mean Reward -2.56 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.376 - Time 2023-07-03T18:57:29\n",
      "Episode 900 - Step 5394 - Epsilon 50.0 - Mean Reward -2.312 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.376 - Time 2023-07-03T18:57:30\n",
      "Episode 920 - Step 5514 - Epsilon 50.0 - Mean Reward -1.91 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.378 - Time 2023-07-03T18:57:30\n",
      "Episode 940 - Step 5634 - Epsilon 50.0 - Mean Reward -1.416 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.381 - Time 2023-07-03T18:57:31\n",
      "Episode 960 - Step 5754 - Epsilon 50.0 - Mean Reward -1.3 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.382 - Time 2023-07-03T18:57:31\n",
      "Guessed START in 5 guesses!\n",
      "Episode 980 - Step 5873 - Epsilon 50.0 - Mean Reward -1.206 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.406 - Time 2023-07-03T18:57:31\n",
      "Episode 1000 - Step 5993 - Epsilon 50.0 - Mean Reward -1.362 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.396 - Time 2023-07-03T18:57:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1020 - Step 6113 - Epsilon 50.0 - Mean Reward -1.76 - Mean Length 5.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.404 - Time 2023-07-03T18:57:32\n",
      "Guessed ALOFT in 3 guesses!\n",
      "Episode 1040 - Step 6230 - Epsilon 50.0 - Mean Reward -1.99 - Mean Length 5.96 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.392 - Time 2023-07-03T18:57:33\n",
      "Episode 1060 - Step 6350 - Epsilon 50.0 - Mean Reward -2.012 - Mean Length 5.96 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.428 - Time 2023-07-03T18:57:33\n",
      "Episode 1080 - Step 6470 - Epsilon 50.0 - Mean Reward -2.02 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.449 - Time 2023-07-03T18:57:33\n",
      "Episode 1100 - Step 6590 - Epsilon 50.0 - Mean Reward -1.934 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.493 - Time 2023-07-03T18:57:34\n",
      "Episode 1120 - Step 6710 - Epsilon 50.0 - Mean Reward -1.806 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.444 - Time 2023-07-03T18:57:34\n",
      "Episode 1140 - Step 6830 - Epsilon 50.0 - Mean Reward -1.866 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.443 - Time 2023-07-03T18:57:35\n",
      "Guessed INTRO in 2 guesses!\n",
      "Episode 1160 - Step 6946 - Epsilon 50.0 - Mean Reward -1.934 - Mean Length 5.96 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.459 - Time 2023-07-03T18:57:35\n",
      "Episode 1180 - Step 7066 - Epsilon 50.0 - Mean Reward -2.196 - Mean Length 5.96 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.497 - Time 2023-07-03T18:57:36\n",
      "Guessed PRONG in 4 guesses!\n",
      "Episode 1200 - Step 7184 - Epsilon 50.0 - Mean Reward -2.172 - Mean Length 5.94 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.455 - Time 2023-07-03T18:57:36\n",
      "Episode 1220 - Step 7304 - Epsilon 50.0 - Mean Reward -2.012 - Mean Length 5.94 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.411 - Time 2023-07-03T18:57:37\n",
      "Episode 1240 - Step 7424 - Epsilon 50.0 - Mean Reward -2.012 - Mean Length 5.94 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.674 - Time 2023-07-03T18:57:37\n",
      "Episode 1260 - Step 7544 - Epsilon 50.0 - Mean Reward -1.93 - Mean Length 5.98 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.427 - Time 2023-07-03T18:57:38\n",
      "Episode 1280 - Step 7664 - Epsilon 50.0 - Mean Reward -1.76 - Mean Length 5.98 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.413 - Time 2023-07-03T18:57:38\n",
      "Episode 1300 - Step 7784 - Epsilon 50.0 - Mean Reward -1.782 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.456 - Time 2023-07-03T18:57:39\n",
      "Episode 1320 - Step 7904 - Epsilon 50.0 - Mean Reward -1.966 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.467 - Time 2023-07-03T18:57:39\n",
      "Episode 1340 - Step 8024 - Epsilon 50.0 - Mean Reward -1.982 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.464 - Time 2023-07-03T18:57:40\n",
      "Episode 1360 - Step 8144 - Epsilon 50.0 - Mean Reward -1.84 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.495 - Time 2023-07-03T18:57:40\n",
      "Guessed SPASM in 1 guesses!\n",
      "Episode 1380 - Step 8259 - Epsilon 50.0 - Mean Reward -1.762 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.466 - Time 2023-07-03T18:57:41\n",
      "Episode 1400 - Step 8379 - Epsilon 50.0 - Mean Reward -1.838 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.453 - Time 2023-07-03T18:57:41\n",
      "Episode 1420 - Step 8499 - Epsilon 50.0 - Mean Reward -1.976 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.452 - Time 2023-07-03T18:57:41\n",
      "Episode 1440 - Step 8619 - Epsilon 50.0 - Mean Reward -2.126 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.446 - Time 2023-07-03T18:57:42\n",
      "Episode 1460 - Step 8739 - Epsilon 50.0 - Mean Reward -2.106 - Mean Length 5.95 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.452 - Time 2023-07-03T18:57:42\n",
      "Episode 1480 - Step 8859 - Epsilon 50.0 - Mean Reward -2.138 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.418 - Time 2023-07-03T18:57:43\n",
      "Episode 1500 - Step 8979 - Epsilon 50.0 - Mean Reward -2.022 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.418 - Time 2023-07-03T18:57:43\n",
      "Episode 1520 - Step 9099 - Epsilon 50.0 - Mean Reward -1.886 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.418 - Time 2023-07-03T18:57:44\n",
      "Episode 1540 - Step 9219 - Epsilon 50.0 - Mean Reward -1.548 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.419 - Time 2023-07-03T18:57:44\n",
      "Episode 1560 - Step 9339 - Epsilon 50.0 - Mean Reward -1.558 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.688 - Time 2023-07-03T18:57:45\n",
      "Episode 1580 - Step 9459 - Epsilon 50.0 - Mean Reward -1.394 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.477 - Time 2023-07-03T18:57:45\n",
      "Episode 1600 - Step 9579 - Epsilon 50.0 - Mean Reward -1.286 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.49 - Time 2023-07-03T18:57:46\n",
      "Episode 1620 - Step 9699 - Epsilon 50.0 - Mean Reward -1.398 - Mean Length 6.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.499 - Time 2023-07-03T18:57:46\n",
      "Guessed LIGHT in 3 guesses!\n",
      "Episode 1640 - Step 9816 - Epsilon 50.0 - Mean Reward -1.496 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.494 - Time 2023-07-03T18:57:47\n",
      "Episode 1660 - Step 9936 - Epsilon 50.0 - Mean Reward -1.664 - Mean Length 5.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.428 - Time 2023-07-03T18:57:47\n",
      "Episode 1680 - Step 10056 - Epsilon 50.0 - Mean Reward -1.79 - Mean Length 5.97 - Mean Loss 0.079 - Mean Q Value -0.001 - Time Delta 0.989 - Time 2023-07-03T18:57:48\n",
      "Episode 1700 - Step 10176 - Epsilon 50.0 - Mean Reward -2.054 - Mean Length 5.97 - Mean Loss 0.242 - Mean Q Value 0.001 - Time Delta 1.665 - Time 2023-07-03T18:57:50\n",
      "Episode 1720 - Step 10296 - Epsilon 50.0 - Mean Reward -2.2 - Mean Length 5.97 - Mean Loss 0.401 - Mean Q Value -0.001 - Time Delta 1.529 - Time 2023-07-03T18:57:51\n",
      "Episode 1740 - Step 10416 - Epsilon 50.0 - Mean Reward -2.058 - Mean Length 6.0 - Mean Loss 0.558 - Mean Q Value -0.001 - Time Delta 1.476 - Time 2023-07-03T18:57:53\n",
      "Episode 1760 - Step 10536 - Epsilon 50.0 - Mean Reward -1.944 - Mean Length 6.0 - Mean Loss 0.71 - Mean Q Value -0.002 - Time Delta 1.405 - Time 2023-07-03T18:57:54\n",
      "Episode 1780 - Step 10656 - Epsilon 50.0 - Mean Reward -1.874 - Mean Length 6.0 - Mean Loss 0.782 - Mean Q Value -0.005 - Time Delta 2.279 - Time 2023-07-03T18:57:56\n",
      "Episode 1800 - Step 10776 - Epsilon 50.0 - Mean Reward -1.954 - Mean Length 6.0 - Mean Loss 0.77 - Mean Q Value -0.01 - Time Delta 1.486 - Time 2023-07-03T18:57:58\n",
      "Episode 1820 - Step 10896 - Epsilon 50.0 - Mean Reward -1.91 - Mean Length 6.0 - Mean Loss 0.76 - Mean Q Value -0.012 - Time Delta 1.78 - Time 2023-07-03T18:58:00\n",
      "Episode 1840 - Step 11016 - Epsilon 50.0 - Mean Reward -1.656 - Mean Length 6.0 - Mean Loss 0.751 - Mean Q Value -0.018 - Time Delta 1.271 - Time 2023-07-03T18:58:01\n",
      "Episode 1860 - Step 11136 - Epsilon 50.0 - Mean Reward -1.914 - Mean Length 6.0 - Mean Loss 0.754 - Mean Q Value -0.024 - Time Delta 1.935 - Time 2023-07-03T18:58:03\n",
      "Episode 1880 - Step 11256 - Epsilon 50.0 - Mean Reward -2.034 - Mean Length 6.0 - Mean Loss 0.748 - Mean Q Value -0.022 - Time Delta 2.273 - Time 2023-07-03T18:58:05\n",
      "Episode 1900 - Step 11376 - Epsilon 50.0 - Mean Reward -1.938 - Mean Length 6.0 - Mean Loss 0.743 - Mean Q Value -0.023 - Time Delta 1.374 - Time 2023-07-03T18:58:07\n",
      "Episode 1920 - Step 11496 - Epsilon 50.0 - Mean Reward -1.754 - Mean Length 6.0 - Mean Loss 0.731 - Mean Q Value -0.033 - Time Delta 1.62 - Time 2023-07-03T18:58:08\n",
      "Episode 1940 - Step 11616 - Epsilon 50.0 - Mean Reward -2.2 - Mean Length 6.0 - Mean Loss 0.711 - Mean Q Value -0.026 - Time Delta 1.556 - Time 2023-07-03T18:58:10\n",
      "Episode 1960 - Step 11736 - Epsilon 50.0 - Mean Reward -2.196 - Mean Length 6.0 - Mean Loss 0.686 - Mean Q Value -0.035 - Time Delta 1.41 - Time 2023-07-03T18:58:11\n",
      "Episode 1980 - Step 11856 - Epsilon 50.0 - Mean Reward -2.04 - Mean Length 6.0 - Mean Loss 0.659 - Mean Q Value -0.057 - Time Delta 1.818 - Time 2023-07-03T18:58:13\n",
      "Episode 2000 - Step 11976 - Epsilon 50.0 - Mean Reward -1.976 - Mean Length 6.0 - Mean Loss 0.639 - Mean Q Value -0.067 - Time Delta 1.194 - Time 2023-07-03T18:58:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2020 - Step 12096 - Epsilon 50.0 - Mean Reward -2.186 - Mean Length 6.0 - Mean Loss 0.614 - Mean Q Value -0.07 - Time Delta 1.68 - Time 2023-07-03T18:58:16\n",
      "Episode 2040 - Step 12216 - Epsilon 50.0 - Mean Reward -2.308 - Mean Length 6.0 - Mean Loss 0.601 - Mean Q Value -0.096 - Time Delta 1.563 - Time 2023-07-03T18:58:17\n",
      "Episode 2060 - Step 12336 - Epsilon 50.0 - Mean Reward -2.292 - Mean Length 6.0 - Mean Loss 0.576 - Mean Q Value -0.102 - Time Delta 1.735 - Time 2023-07-03T18:58:19\n",
      "Episode 2080 - Step 12456 - Epsilon 50.0 - Mean Reward -2.432 - Mean Length 6.0 - Mean Loss 0.574 - Mean Q Value -0.111 - Time Delta 2.472 - Time 2023-07-03T18:58:22\n",
      "Episode 2100 - Step 12576 - Epsilon 50.0 - Mean Reward -2.278 - Mean Length 6.0 - Mean Loss 0.552 - Mean Q Value -0.128 - Time Delta 1.583 - Time 2023-07-03T18:58:23\n",
      "Episode 2120 - Step 12696 - Epsilon 50.0 - Mean Reward -2.112 - Mean Length 6.0 - Mean Loss 0.543 - Mean Q Value -0.148 - Time Delta 1.827 - Time 2023-07-03T18:58:25\n",
      "Episode 2140 - Step 12816 - Epsilon 50.0 - Mean Reward -2.018 - Mean Length 6.0 - Mean Loss 0.524 - Mean Q Value -0.151 - Time Delta 1.714 - Time 2023-07-03T18:58:27\n",
      "Episode 2160 - Step 12936 - Epsilon 50.0 - Mean Reward -1.914 - Mean Length 6.0 - Mean Loss 0.523 - Mean Q Value -0.153 - Time Delta 2.157 - Time 2023-07-03T18:58:29\n",
      "Episode 2180 - Step 13056 - Epsilon 50.0 - Mean Reward -1.894 - Mean Length 6.0 - Mean Loss 0.504 - Mean Q Value -0.142 - Time Delta 2.617 - Time 2023-07-03T18:58:31\n",
      "Episode 2200 - Step 13176 - Epsilon 50.0 - Mean Reward -1.89 - Mean Length 6.0 - Mean Loss 0.5 - Mean Q Value -0.128 - Time Delta 2.09 - Time 2023-07-03T18:58:34\n",
      "Episode 2220 - Step 13296 - Epsilon 50.0 - Mean Reward -1.77 - Mean Length 6.0 - Mean Loss 0.484 - Mean Q Value -0.114 - Time Delta 2.66 - Time 2023-07-03T18:58:36\n",
      "Episode 2240 - Step 13416 - Epsilon 50.0 - Mean Reward -1.674 - Mean Length 6.0 - Mean Loss 0.485 - Mean Q Value -0.118 - Time Delta 2.729 - Time 2023-07-03T18:58:39\n",
      "Episode 2260 - Step 13536 - Epsilon 50.0 - Mean Reward -1.668 - Mean Length 6.0 - Mean Loss 0.477 - Mean Q Value -0.12 - Time Delta 2.231 - Time 2023-07-03T18:58:41\n",
      "Episode 2280 - Step 13656 - Epsilon 50.0 - Mean Reward -1.552 - Mean Length 6.0 - Mean Loss 0.473 - Mean Q Value -0.128 - Time Delta 2.452 - Time 2023-07-03T18:58:44\n",
      "Episode 2300 - Step 13776 - Epsilon 50.0 - Mean Reward -1.846 - Mean Length 6.0 - Mean Loss 0.459 - Mean Q Value -0.145 - Time Delta 2.471 - Time 2023-07-03T18:58:46\n",
      "Episode 2320 - Step 13896 - Epsilon 50.0 - Mean Reward -1.846 - Mean Length 6.0 - Mean Loss 0.462 - Mean Q Value -0.152 - Time Delta 2.639 - Time 2023-07-03T18:58:49\n",
      "Episode 2340 - Step 14016 - Epsilon 50.0 - Mean Reward -2.138 - Mean Length 6.0 - Mean Loss 0.448 - Mean Q Value -0.145 - Time Delta 1.911 - Time 2023-07-03T18:58:51\n",
      "Episode 2360 - Step 14136 - Epsilon 50.0 - Mean Reward -2.222 - Mean Length 6.0 - Mean Loss 0.44 - Mean Q Value -0.154 - Time Delta 1.982 - Time 2023-07-03T18:58:53\n",
      "Guessed AWARE in 5 guesses!\n",
      "Episode 2380 - Step 14255 - Epsilon 50.0 - Mean Reward -2.236 - Mean Length 5.99 - Mean Loss 0.433 - Mean Q Value -0.144 - Time Delta 2.147 - Time 2023-07-03T18:58:55\n",
      "Episode 2400 - Step 14375 - Epsilon 50.0 - Mean Reward -2.082 - Mean Length 5.99 - Mean Loss 0.421 - Mean Q Value -0.158 - Time Delta 2.116 - Time 2023-07-03T18:58:57\n",
      "Episode 2420 - Step 14495 - Epsilon 50.0 - Mean Reward -2.11 - Mean Length 5.99 - Mean Loss 0.421 - Mean Q Value -0.168 - Time Delta 2.34 - Time 2023-07-03T18:58:59\n",
      "Episode 2440 - Step 14615 - Epsilon 50.0 - Mean Reward -1.798 - Mean Length 5.99 - Mean Loss 0.422 - Mean Q Value -0.185 - Time Delta 2.936 - Time 2023-07-03T18:59:02\n",
      "Episode 2460 - Step 14735 - Epsilon 50.0 - Mean Reward -1.644 - Mean Length 5.99 - Mean Loss 0.419 - Mean Q Value -0.199 - Time Delta 2.451 - Time 2023-07-03T18:59:05\n",
      "Episode 2480 - Step 14855 - Epsilon 50.0 - Mean Reward -1.782 - Mean Length 6.0 - Mean Loss 0.419 - Mean Q Value -0.216 - Time Delta 3.469 - Time 2023-07-03T18:59:08\n",
      "Episode 2500 - Step 14975 - Epsilon 50.0 - Mean Reward -1.61 - Mean Length 6.0 - Mean Loss 0.428 - Mean Q Value -0.2 - Time Delta 2.223 - Time 2023-07-03T18:59:10\n",
      "Guessed EERIE in 6 guesses!\n",
      "Episode 2520 - Step 15095 - Epsilon 50.0 - Mean Reward -1.706 - Mean Length 6.0 - Mean Loss 0.426 - Mean Q Value -0.205 - Time Delta 2.382 - Time 2023-07-03T18:59:13\n",
      "Episode 2540 - Step 15215 - Epsilon 50.0 - Mean Reward -1.798 - Mean Length 6.0 - Mean Loss 0.421 - Mean Q Value -0.214 - Time Delta 2.083 - Time 2023-07-03T18:59:15\n",
      "Episode 2560 - Step 15335 - Epsilon 50.0 - Mean Reward -1.964 - Mean Length 6.0 - Mean Loss 0.416 - Mean Q Value -0.209 - Time Delta 2.538 - Time 2023-07-03T18:59:17\n",
      "Episode 2580 - Step 15455 - Epsilon 50.0 - Mean Reward -1.912 - Mean Length 6.0 - Mean Loss 0.404 - Mean Q Value -0.219 - Time Delta 2.098 - Time 2023-07-03T18:59:19\n",
      "Guessed RUGBY in 1 guesses!\n",
      "Episode 2600 - Step 15570 - Epsilon 50.0 - Mean Reward -2.062 - Mean Length 5.95 - Mean Loss 0.405 - Mean Q Value -0.228 - Time Delta 2.37 - Time 2023-07-03T18:59:22\n",
      "Episode 2620 - Step 15690 - Epsilon 50.0 - Mean Reward -2.0 - Mean Length 5.95 - Mean Loss 0.404 - Mean Q Value -0.222 - Time Delta 2.89 - Time 2023-07-03T18:59:25\n",
      "Episode 2640 - Step 15810 - Epsilon 50.0 - Mean Reward -1.85 - Mean Length 5.95 - Mean Loss 0.408 - Mean Q Value -0.212 - Time Delta 2.131 - Time 2023-07-03T18:59:27\n",
      "Episode 2660 - Step 15930 - Epsilon 50.0 - Mean Reward -1.66 - Mean Length 5.95 - Mean Loss 0.41 - Mean Q Value -0.216 - Time Delta 3.027 - Time 2023-07-03T18:59:30\n",
      "Episode 2680 - Step 16050 - Epsilon 50.0 - Mean Reward -1.516 - Mean Length 5.95 - Mean Loss 0.415 - Mean Q Value -0.217 - Time Delta 2.269 - Time 2023-07-03T18:59:32\n",
      "Episode 2700 - Step 16170 - Epsilon 50.0 - Mean Reward -1.684 - Mean Length 6.0 - Mean Loss 0.414 - Mean Q Value -0.233 - Time Delta 2.464 - Time 2023-07-03T18:59:35\n",
      "Episode 2720 - Step 16290 - Epsilon 50.0 - Mean Reward -1.876 - Mean Length 6.0 - Mean Loss 0.4 - Mean Q Value -0.261 - Time Delta 2.112 - Time 2023-07-03T18:59:37\n",
      "Episode 2740 - Step 16410 - Epsilon 50.0 - Mean Reward -2.214 - Mean Length 6.0 - Mean Loss 0.392 - Mean Q Value -0.268 - Time Delta 2.132 - Time 2023-07-03T18:59:39\n",
      "Episode 2760 - Step 16530 - Epsilon 50.0 - Mean Reward -2.164 - Mean Length 6.0 - Mean Loss 0.397 - Mean Q Value -0.275 - Time Delta 3.215 - Time 2023-07-03T18:59:42\n",
      "Episode 2780 - Step 16650 - Epsilon 50.0 - Mean Reward -2.276 - Mean Length 6.0 - Mean Loss 0.397 - Mean Q Value -0.284 - Time Delta 2.151 - Time 2023-07-03T18:59:44\n",
      "Episode 2800 - Step 16770 - Epsilon 50.0 - Mean Reward -2.106 - Mean Length 6.0 - Mean Loss 0.398 - Mean Q Value -0.281 - Time Delta 2.102 - Time 2023-07-03T18:59:46\n",
      "Episode 2820 - Step 16890 - Epsilon 50.0 - Mean Reward -1.918 - Mean Length 6.0 - Mean Loss 0.404 - Mean Q Value -0.267 - Time Delta 1.883 - Time 2023-07-03T18:59:48\n",
      "Episode 2840 - Step 17010 - Epsilon 50.0 - Mean Reward -1.836 - Mean Length 6.0 - Mean Loss 0.408 - Mean Q Value -0.269 - Time Delta 1.888 - Time 2023-07-03T18:59:50\n",
      "Episode 2860 - Step 17130 - Epsilon 50.0 - Mean Reward -2.186 - Mean Length 6.0 - Mean Loss 0.403 - Mean Q Value -0.269 - Time Delta 1.965 - Time 2023-07-03T18:59:52\n",
      "Episode 2880 - Step 17250 - Epsilon 50.0 - Mean Reward -2.026 - Mean Length 6.0 - Mean Loss 0.398 - Mean Q Value -0.272 - Time Delta 2.435 - Time 2023-07-03T18:59:54\n",
      "Episode 2900 - Step 17370 - Epsilon 50.0 - Mean Reward -1.894 - Mean Length 6.0 - Mean Loss 0.392 - Mean Q Value -0.262 - Time Delta 2.371 - Time 2023-07-03T18:59:57\n",
      "Episode 2920 - Step 17490 - Epsilon 50.0 - Mean Reward -1.686 - Mean Length 6.0 - Mean Loss 0.386 - Mean Q Value -0.259 - Time Delta 2.44 - Time 2023-07-03T18:59:59\n",
      "Episode 2940 - Step 17610 - Epsilon 50.0 - Mean Reward -1.558 - Mean Length 6.0 - Mean Loss 0.382 - Mean Q Value -0.262 - Time Delta 2.126 - Time 2023-07-03T19:00:01\n",
      "Episode 2960 - Step 17730 - Epsilon 50.0 - Mean Reward -1.344 - Mean Length 6.0 - Mean Loss 0.375 - Mean Q Value -0.252 - Time Delta 2.068 - Time 2023-07-03T19:00:03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m Wordie\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m \u001b[43mWordie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m     33\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_step(reward, loss, q)\n",
      "Cell \u001b[0;32mIn[23], line 121\u001b[0m, in \u001b[0;36mWordleAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m td_tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_target(reward, next_state, done)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Backpropagate loss through Q_online\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_Q_online\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd_tgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (td_est\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(), loss)\n",
      "Cell \u001b[0;32mIn[23], line 89\u001b[0m, in \u001b[0;36mWordleAgent.update_Q_online\u001b[0;34m(self, td_estimate, td_target)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "Wordie = WordleAgent(state_dim=(1,12,5),action_dim=actions,save_dir=save_dir)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "\n",
    "episodes = 50000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = Wordie.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, dic = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        Wordie.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = Wordie.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "    \n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=Wordie.exploration_rate, step=Wordie.curr_step)\n",
    "print(env.guessed_words)\n",
    "print(env.WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "# Play the game!\n",
    "while True:\n",
    "\n",
    "    # Run agent on the state\n",
    "    action = Wordie.act(state)\n",
    "\n",
    "    # Agent performs action\n",
    "    next_state, reward, done, dic = env.step(action)\n",
    "\n",
    "    # Remember\n",
    "    Wordie.cache(state, next_state, action, reward, done)\n",
    "\n",
    "    # Learn\n",
    "    q, loss = Wordie.learn()\n",
    "\n",
    "    # Logging\n",
    "    logger.log_step(reward, loss, q)\n",
    "\n",
    "    # Update state\n",
    "    state = next_state\n",
    "\n",
    "    \n",
    "    # Check if end of game\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "logger.log_episode()\n",
    "\n",
    "if e % 20 == 0:\n",
    "    logger.record(episode=e, epsilon=Wordie.exploration_rate, step=Wordie.curr_step)\n",
    "\n",
    "print(env.guessed_words)\n",
    "print(env.WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
